import time
import random
import dns.resolver
from dotenv import load_dotenv
from telethon import TelegramClient, events, errors
import asyncio
import os
import re
import logging
import json
from datetime import datetime, timezone, timedelta
import httpx
import sqlite3
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import defaultdict
from deep_translator import GoogleTranslator
import hashlib
import langdetect
from openai import AsyncOpenAI
from prometheus_client import start_http_server, Counter, Gauge

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ DNS —Ä–µ–∑–æ–ª–≤–µ—Ä–∞
dns.resolver.default_resolver = dns.resolver.Resolver(configure=False)
dns.resolver.default_resolver.nameservers = ['8.8.8.8', '1.1.1.1']  # Google –∏ Cloudflare DNS

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞ HTML
try:
    import lxml
    DEFAULT_PARSER = "lxml"
except ImportError:
    DEFAULT_PARSER = "html.parser"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("autopost.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ .env
load_dotenv()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
BOT_TOKEN_COLLECTOR = os.getenv("BOT_TOKEN_COLLECTOR")
BOT_TOKEN_PUBLISHER = os.getenv("BOT_TOKEN_PUBLISHER")
API_ID = int(os.getenv("API_ID", 0))
API_HASH = os.getenv("API_HASH")
CHANNEL = os.getenv("CHANNEL")
PROXY_HTTP = os.getenv("PROXY_HTTP", "").strip()
PROXY_HTTPS = os.getenv("PROXY_HTTPS", "").strip()
ADMIN_CHAT_ID = os.getenv("ADMIN_CHAT_ID")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
required_vars = [
    "BOT_TOKEN_COLLECTOR", 
    "BOT_TOKEN_PUBLISHER", 
    "API_ID", 
    "API_HASH", 
    "CHANNEL", 
    "ADMIN_CHAT_ID"
]

missing = [var for var in required_vars if not os.getenv(var)]
if missing:
    logger.error(f"–ù–µ –≤—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã! –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {', '.join(missing)}")
    exit(1)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
SOURCES_CONFIG = [
    {
        "name": "Forbes –†–æ—Å—Å–∏—è",
        "url": "https://www.forbes.ru/finansy/",
        "rss_url": "https://www.forbes.ru/newrss.xml",
        "category": "—Ñ–∏–Ω–∞–Ω—Å—ã",
        "tags": ["—ç–∫–æ–Ω–æ–º–∏–∫–∞", "–±–∏–∑–Ω–µ—Å", "—Ä—ã–Ω–∫–∏"],
        "lang": "ru",
        "trust_score": 9
    },
    {
        "name": "Investing.com (Russian)",
        "url": "https://ru.investing.com/",
        "rss_url": "https://ru.investing.com/rss/news.rss",
        "category": "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏",
        "tags": ["–∞–∫—Ü–∏–∏", "–æ–±–ª–∏–≥–∞—Ü–∏–∏", "—Ç—Ä–µ–π–¥–∏–Ω–≥"],
        "lang": "ru",
        "trust_score": 8
    },
    {
        "name": "CoinDesk (Russian)",
        "url": "https://www.coindesk.com/ru/",
        "rss_url": "https://www.coindesk.com/ru/arc/outboundfeeds/rss/?outputType=xml",
        "category": "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã",
        "tags": ["–±–∏—Ç–∫–æ–∏–Ω", "–±–ª–æ–∫—á–µ–π–Ω", "NFT"],
        "lang": "ru",
        "trust_score": 7
    },
    {
        "name": "–†–ë–ö",
        "url": "https://www.rbc.ru/",
        "rss_url": "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
        "category": "—ç–∫–æ–Ω–æ–º–∏–∫–∞",
        "tags": ["–±–∏–∑–Ω–µ—Å", "—Ä—ã–Ω–∫–∏", "—Ñ–∏–Ω–∞–Ω—Å—ã"],
        "lang": "ru",
        "trust_score": 8
    },
    {
        "name": "–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä",
        "url": "https://www.kommersant.ru/",
        "rss_url": "https://www.kommersant.ru/RSS/news.xml",
        "category": "—ç–∫–æ–Ω–æ–º–∏–∫–∞",
        "tags": ["–ø–æ–ª–∏—Ç–∏–∫–∞", "–±–∏–∑–Ω–µ—Å", "—Ñ–∏–Ω–∞–Ω—Å—ã"],
        "lang": "ru",
        "trust_score": 9
    }
]

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
class NewsDatabase:
    def __init__(self, db_path="news_bot.db"):
        self.db_path = db_path
        self.conn = None
        self.initialize_db()
        
    def initialize_db(self):
        self.conn = sqlite3.connect(self.db_path)
        cursor = self.conn.cursor()
        
        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ SQL-–∑–∞–ø—Ä–æ—Å—ã:
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS posted_links (
                url TEXT PRIMARY KEY,
                timestamp DATETIME
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS content_hashes (
                hash TEXT PRIMARY KEY,
                timestamp DATETIME
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS title_hashes (
                hash TEXT PRIMARY KEY,
                timestamp DATETIME
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS analytics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT,
                title TEXT,
                category TEXT,
                tags TEXT,
                importance INTEGER,
                source TEXT,
                content_hash TEXT,
                title_hash TEXT,
                timestamp DATETIME,
                summary TEXT,
                trust_score INTEGER,
                engagement REAL DEFAULT 0
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS translation_cache (
                source_text TEXT PRIMARY KEY,
                translated_text TEXT,
                timestamp DATETIME
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS pending_links (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT,
                source TEXT,
                added_by INTEGER,
                timestamp DATETIME,
                priority INTEGER DEFAULT 1
            )
        ''')
        
        self.conn.commit()
    
    def add_posted_link(self, url):
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT OR IGNORE INTO posted_links (url, timestamp)
            VALUES (?, ?)
        ''', (url, datetime.now(timezone.utc).isoformat()))
        self.conn.commit()
    
    def is_link_posted(self, url):
        cursor = self.conn.cursor()
        cursor.execute("SELECT 1 FROM posted_links WHERE url = ?", (url,))
        return cursor.fetchone() is not None
    
    def add_content_hash(self, content_hash):
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT OR IGNORE INTO content_hashes (hash, timestamp)
            VALUES (?, ?)
        ''', (content_hash, datetime.now(timezone.utc).isoformat()))
        self.conn.commit()
    
    def is_content_hash_exists(self, content_hash):
        cursor = self.conn.cursor()
        cursor.execute("SELECT 1 FROM content_hashes WHERE hash = ?", (content_hash,))
        return cursor.fetchone() is not None
    
    def add_title_hash(self, title_hash):
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT OR IGNORE INTO title_hashes (hash, timestamp)
            VALUES (?, ?)
        ''', (title_hash, datetime.now(timezone.utc).isoformat()))
        self.conn.commit()
    
    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥:
    def is_title_hash_recent(self, title_hash, days=3):
        cursor = self.conn.cursor()
        time_threshold = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()
        cursor.execute('''
            SELECT 1 FROM title_hashes 
            WHERE hash = ? AND timestamp > ?
        ''', (title_hash, time_threshold))
        return cursor.fetchone() is not None
    
    def add_analytics(self, analytics_data):
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT INTO analytics (
                url, title, category, tags, importance, source, 
                content_hash, title_hash, timestamp, summary, trust_score
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            analytics_data['url'],
            analytics_data['title'],
            analytics_data['category'],
            json.dumps(analytics_data['tags']),
            analytics_data['importance_level'],
            analytics_data['source'],
            analytics_data['content_hash'],
            analytics_data['title_hash'],
            analytics_data['timestamp'],
            analytics_data.get('summary', ''),
            analytics_data.get('trust_score', 5)
        ))
        self.conn.commit()
    
    def get_recent_analytics(self, hours=12):
        cursor = self.conn.cursor()
        time_threshold = (datetime.now(timezone.utc) - timedelta(hours=hours)).isoformat()
        cursor.execute('''
            SELECT * FROM analytics 
            WHERE timestamp > ?
        ''', (time_threshold,))
        columns = [col[0] for col in cursor.description]
        return [dict(zip(columns, row)) for row in cursor.fetchall()]
    
    def get_category_stats(self, hours=12):
        cursor = self.conn.cursor()
        time_threshold = (datetime.now(timezone.utc) - timedelta(hours=hours)).isoformat()
        cursor.execute('''
            SELECT category, COUNT(*) as count 
            FROM analytics 
            WHERE timestamp > ?
            GROUP BY category
        ''', (time_threshold,))
        return {row[0]: row[1] for row in cursor.fetchall()}
    
    def get_keyword_frequency(self, hours=12, limit=5):
        cursor = self.conn.cursor()
        time_threshold = (datetime.now(timezone.utc) - timedelta(hours=hours)).isoformat()
        cursor.execute('''
            SELECT value, COUNT(*) as count
            FROM analytics, json_each(tags)
            WHERE timestamp > ?
            GROUP BY value
            ORDER BY count DESC
            LIMIT ?
        ''', (time_threshold, limit))
        return cursor.fetchall()
    
    def add_pending_link(self, url, source, added_by=None, priority=1):
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT INTO pending_links (url, source, added_by, timestamp, priority)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            url, 
            source, 
            added_by, 
            datetime.now(timezone.utc).isoformat(),
            priority
        ))
        self.conn.commit()
        return cursor.lastrowid
    
    def get_pending_links(self, limit=15, priority_threshold=0):
        cursor = self.conn.cursor()
        cursor.execute('''
            SELECT * FROM pending_links 
            WHERE priority >= ?
            ORDER BY priority DESC, timestamp ASC
            LIMIT ?
        ''', (priority_threshold, limit))
        columns = [col[0] for col in cursor.description]
        return [dict(zip(columns, row)) for row in cursor.fetchall()]
    
    def remove_pending_link(self, link_id):
        cursor = self.conn.cursor()
        cursor.execute("DELETE FROM pending_links WHERE id = ?", (link_id,))
        self.conn.commit()
    
    def clear_pending_links(self):
        cursor = self.conn.cursor()
        cursor.execute("DELETE FROM pending_links")
        self.conn.commit()
    
    def get_translation(self, source_text):
        cursor = self.conn.cursor()
        cursor.execute("SELECT translated_text FROM translation_cache WHERE source_text = ?", (source_text,))
        result = cursor.fetchone()
        return result[0] if result else None
    
    def add_translation(self, source_text, translated_text):
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT OR REPLACE INTO translation_cache (source_text, translated_text, timestamp)
            VALUES (?, ?, ?)
        ''', (source_text, translated_text, datetime.now(timezone.utc).isoformat()))
        self.conn.commit()
    
    def update_engagement(self, url, engagement_score):
        cursor = self.conn.cursor()
        cursor.execute('''
            UPDATE analytics 
            SET engagement = engagement + ?
            WHERE url = ?
        ''', (engagement_score, url))
        self.conn.commit()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
db = NewsDatabase()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Prometheus –º–µ—Ç—Ä–∏–∫
REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests')
PARSE_SUCCESS = Counter('parse_success_total', 'Successful article parses')
PARSE_FAILURE = Counter('parse_failure_total', 'Failed article parses')
PUBLISH_COUNT = Counter('publish_count_total', 'Total published articles')
QUEUE_SIZE = Gauge('pending_queue_size', 'Current pending links queue size')

# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –º–µ—Ç—Ä–∏–∫
start_http_server(8000)

# ================== –£–¢–ò–õ–ò–¢–´ –ò –û–ë–©–ò–ï –§–£–ù–ö–¶–ò–ò ==================

def escape_markdown(text):
    """–≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ Markdown"""
    if not text:
        return text
        
    escape_chars = r'_*[]()~`>#+-=|{}.!'
    return re.sub(f'([{re.escape(escape_chars)}])', r'\\\1', text)

def is_valid_url(url):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL"""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc]) and result.scheme in ['http', 'https']
    except:
        return False

def validate_proxy_url(proxy_url):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL –ø—Ä–æ–∫—Å–∏"""
    if not proxy_url:
        return False
    
    try:
        parsed = urlparse(proxy_url)
        if not parsed.scheme or not parsed.netloc:
            return False
            
        if parsed.scheme not in ('http', 'https', 'socks5'):
            return False
            
        return True
    except Exception:
        return False

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∫—Å–∏
PROXIES = {}
if validate_proxy_url(PROXY_HTTP):
    PROXIES['http://'] = PROXY_HTTP
if validate_proxy_url(PROXY_HTTPS):
    PROXIES['https://'] = PROXY_HTTPS

if PROXIES:
    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–æ–∫—Å–∏: {PROXIES}")
else:
    logger.info("–ü—Ä–æ–∫—Å–∏ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –∏–ª–∏ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã, —Ä–∞–±–æ—Ç–∞–µ–º –±–µ–∑ –Ω–∏—Ö")

# –°–ø–∏—Å–æ–∫ User-Agent –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; rv:124.0) Gecko/20100101 Firefox/124.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 17_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1',
]

# –°–ø–∏—Å–æ–∫ —Ä–µ—Ñ–µ—Ä–µ—Ä–æ–≤
REFERERS = [
    "https://www.google.com/",
    "https://yandex.ru/",
    "https://duckduckgo.com/",
    "https://www.bing.com/",
    "https://www.facebook.com/",
    "https://twitter.com/",
]

async def create_async_session():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Å–µ—Å—Å–∏–∏ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
    limits = httpx.Limits(max_connections=100, max_keepalive_connections=20)
    timeout = httpx.Timeout(30.0, connect=10.0)
    
    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∫–ª–∏–µ–Ω—Ç–∞
    client_params = {
        'limits': limits,
        'timeout': timeout,
        'follow_redirects': True,
        'http2': True,
        'headers': {
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Connection': 'keep-alive',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–∫—Å–∏ –µ—Å–ª–∏ –æ–Ω–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã
    if PROXIES:
        client_params['proxies'] = PROXIES
    
    return httpx.AsyncClient(**client_params)

def detect_language(text):
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    try:
        return langdetect.detect(text) if text else 'en'
    except:
        return 'en'

async def translate_to_russian(text, source_lang='auto'):
    """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    if not text or len(text) < 10:
        return text
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ –∫—ç—à–µ
    cached = db.get_translation(text)
    if cached:
        return cached
    
    try:
        # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
        if source_lang == 'auto':
            source_lang = detect_language(text)
            
        if source_lang == 'ru':
            return text
            
        text = text[:3000]
        
        translator = GoogleTranslator(source=source_lang, target='ru')
        translated = translator.translate(text)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
        db.add_translation(text, translated)
        return translated
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
        return text

def extract_keywords(text, num_keywords=5):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    try:
        stop_words = set(stopwords.words('russian') + stopwords.words('english'))
        words = word_tokenize(text.lower())
        words = [word for word in words if word.isalnum() and word not in stop_words]
        
        freq_dist = nltk.FreqDist(words)
        return [word for word, _ in freq_dist.most_common(num_keywords)]
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
        return []

def analyze_importance(title, content):
    """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    try:
        urgent_keywords = ["—Å—Ä–æ—á–Ω–æ", "—Å—Ä–æ—á–Ω–∞—è", "–∫—Ä–∏–∑–∏—Å", "–ø–∞–¥–µ–Ω–∏–µ", "—Ä–æ—Å—Ç", "–∞–≤–∞—Ä–∏—è", "—á–ø", "urgent", "crisis", "crash"]
        important_keywords = ["–≤–∞–∂–Ω–æ", "–∏–∑–º–µ–Ω–µ–Ω–∏–µ", "–∑–∞–∫–æ–Ω", "—Ä–µ—à–µ–Ω–∏–µ", "–∏—Ç–æ–≥–∏", "important", "decision", "law"]
        
        text = (title + " " + content).lower()
        
        if any(keyword in text for keyword in urgent_keywords):
            return "üî• –°–†–û–ß–ù–û", 3
        elif any(keyword in text for keyword in important_keywords):
            return "‚ùó –í–ê–ñ–ù–û", 2
        else:
            return "üìå –ò–ù–§–û–†–ú–ê–¶–ò–Ø", 1
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏: {e}")
        return "üìå –ò–ù–§–û–†–ú–ê–¶–ò–Ø", 1

def clean_title(title):
    """–û—á–∏—Å—Ç–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –æ—Ç –º—É—Å–æ—Ä–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
    if not title:
        return title
    
    patterns = [
        r'\s*-\s*Forbes\s*Russia$',
        r'\s*-\s*–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏$',
        r'\s*-\s*CoinDesk$',
        r'\s*:\s*–ì–ª–∞–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏$',
        r'\s*\|.*$',
        r'\s*‚Äî.*$',
        r'\s*‚Äì.*$'
    ]
    
    for pattern in patterns:
        title = re.sub(pattern, "", title, flags=re.IGNORECASE)
    
    return escape_markdown(title.strip())

def clean_content(content, source_name):
    """–û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–Ω—ã—Ö —Ñ—Ä–∞–∑ –∏ –Ω–µ–Ω—É–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
    if not content:
        return content
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –∫—Ä–∏–ø—Ç–æ-–∫–æ—Ç–∏—Ä–æ–≤–æ–∫
    crypto_pattern = r'\b(?:[A-Z]{2,5}\s*\$\s*[\d,]+\.\d{2}\s*[+-]\s*\d+\.\d{2}\s*%\b\s*)+'
    content = re.sub(crypto_pattern, "", content, flags=re.IGNORECASE)
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
    time_patterns = [
        r'\d{1,2}\s*(—á–∞—Å[–∞]?|—á)\s*–Ω–∞–∑–∞–¥',
        r'\d{1,2}\s*(–º–∏–Ω—É—Ç[—ã]?|–º–∏–Ω)\s*–Ω–∞–∑–∞–¥',
        r'\d{1,2}\s*(–¥–µ–Ω—å|–¥–Ω—è|–¥–Ω–µ–π)\s*–Ω–∞–∑–∞–¥',
        r'–≤—á–µ—Ä–∞ –≤ \d{1,2}:\d{2}',
        r'—Å–µ–≥–æ–¥–Ω—è –≤ \d{1,2}:\d{2}',
        r'\d{1,2}\s*[–∞-—è]+\s*–Ω–∞–∑–∞–¥'
    ]
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–Ω—ã—Ö —Ñ—Ä–∞–∑
    junk_phrases = [
        "–†–µ–∂–∏–º –¥–ª—è —Å–ª–∞–±–æ–≤–∏–¥—è—â–∏—Ö",
        "–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ",
        "–¢–µ–ø–µ—Ä—å Forbes –º–æ–∂–Ω–æ —Å–ª—É—à–∞—Ç—å",
        "–ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å",
        "–ü–æ–¥–µ–ª–∏—Ç—å—Å—è",
        "–í –∏–∑–±—Ä–∞–Ω–Ω–æ–µ",
        "–ö–∞—Ä—å–µ—Ä–∞ –∏ —Å–≤–æ–π –±–∏–∑–Ω–µ—Å",
        "–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ",
        "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏",
        "–ò—Å—Ç–æ—á–Ω–∏–∫:",
        "–§–æ—Ç–æ:",
        "–í–∏–¥–µ–æ:",
        "–°–ª–µ–¥–∏—Ç–µ –∑–∞ –Ω–æ–≤–æ—Å—Ç—è–º–∏",
        "–ü–æ–¥–ø–∏—à–∏—Ç–µ—Å—å –Ω–∞",
        "–†–µ–∫–ª–∞–º–∞",
        "–ü–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª"
    ]
    
    # –£–¥–∞–ª–µ–Ω–∏–µ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –±–ª–æ–∫–æ–≤
    ads_patterns = [
        r"ADVERTISEMENT",
        r"–†–µ–∫–ª–∞–º[–∞—ã]",
        r"–ü–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª",
        r"–°–ø–æ–Ω—Å–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞",
        r"Advertise with us"
    ]
    
    # –£–¥–∞–ª–µ–Ω–∏–µ —Å–æ—Ü. —Å–µ—Ç–µ–π
    social_patterns = [
        r"–ü–æ–¥–µ–ª–∏—Ç—å—Å—è –≤ [–ê-–Ø–∞-—èA-Za-z]+",
        r"Share on [A-Za-z]+",
        r"Like us on [A-Za-z]+",
        r"–°–ª–µ–¥–∏—Ç–µ –∑–∞ –Ω–∞–º–∏ –≤ [–ê-–Ø–∞-—è]+"
    ]
    
    # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    source_specific = {
        "Forbes –†–æ—Å—Å–∏—è": [
            r"–¢–µ–≥–∏:.*",
            r"–ê–≤—Ç–æ—Ä:.*",
            r"–§–æ—Ç–æ:.*"
        ],
        "Investing.com": [
            r"–ß–∏—Ç–∞—Ç—å –¥–∞–ª—å—à–µ",
            r"–ü–æ —Ç–µ–º–µ:.*",
            r"–°–≤—è–∑–∞–Ω–Ω—ã–µ —Ä—ã–Ω–∫–∏.*"
        ],
        "CoinDesk": [
            r"Subscribe to.*",
            r"Follow us on.*",
            r"Disclaimer:.*"
        ],
        "–†–ë–ö": [
            r"–ß–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ",
            r"–ê–≤—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞:",
            r"–§–æ—Ç–æ:"
        ],
        "–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä": [
            r"–ü–æ–¥—Ä–æ–±–Ω–µ–µ —á–∏—Ç–∞–π—Ç–µ",
            r"–§–æ—Ç–æ:",
            r"–ò—Å—Ç–æ—á–Ω–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:"
        ]
    }
    
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤
    for pattern in time_patterns + ads_patterns + social_patterns:
        content = re.sub(pattern, "", content, flags=re.IGNORECASE)
    
    for phrase in junk_phrases:
        content = content.replace(phrase, "")
    
    for source, patterns in source_specific.items():
        if source_name in source:
            for pattern in patterns:
                content = re.sub(pattern, "", content, flags=re.IGNORECASE)
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
    content = re.sub(r"^\s*[\d–ê-–Ø–∞-—è]+\s*–Ω–∞–∑–∞–¥\W*", "", content)
    content = re.sub(r'(?:\s*\n\s*){2,}', '\n\n', content)
    content = re.sub(r'\s+', ' ', content).strip()
    
    return escape_markdown(content)

def extract_main_content(soup):
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç—å–∏"""
    content_selectors = [
        {'selector': 'article', 'attribute': 'text'},
        {'selector': 'div.post-content', 'attribute': 'text'},
        {'selector': 'div.article-content', 'attribute': 'text'},
        {'selector': 'div.content', 'attribute': 'text'},
        {'selector': 'div.entry-content', 'attribute': 'text'},
        {'selector': 'div.story-block', 'attribute': 'text'},
        {'selector': 'main', 'attribute': 'text'},
        {'selector': '[itemprop="articleBody"]', 'attribute': 'text'},
        {'selector': '.article-body', 'attribute': 'text'},
        {'selector': '.post-body', 'attribute': 'text'},
        {'selector': '.text', 'attribute': 'text'},
        {'selector': '.content-area', 'attribute': 'text'},
        {'selector': 'div.article__text', 'attribute': 'text'},  # –†–ë–ö
        {'selector': 'div.article-text', 'attribute': 'text'},   # –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç
    ]
    
    for selector in content_selectors:
        element = soup.select_one(selector['selector'])
        if element:
            if selector['attribute'] == 'text':
                content = element.get_text(separator='\n', strip=True)
            else:
                content = element.get(selector['attribute'], '')
            if content and len(content) > 300:
                return content
    
    return extract_content_algorithmically(soup)

def extract_content_algorithmically(soup):
    """–ê–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
    for element in soup(['script', 'style', 'header', 'footer', 'aside', 'nav']):
        element.decompose()
    
    # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–∫—Å—Ç–∞
    body = soup.body
    if body:
        # –í—ã—á–∏—Å–ª—è–µ–º –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
        elements = body.find_all(True)
        best_element = None
        best_density = 0
        
        for element in elements:
            text = element.get_text()
            text_length = len(text)
            html_length = len(str(element))
            
            if html_length > 0:
                density = text_length / html_length
                if density > best_density and text_length > 200:
                    best_density = density
                    best_element = element
        
        if best_element:
            return best_element.get_text(separator='\n', strip=True)
    
    # –§–æ–ª–±—ç–∫: —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
    paragraphs = soup.find_all('p')
    texts = []
    for p in paragraphs:
        text = p.get_text(strip=True)
        if text:
            texts.append(text)
    return '\n\n'.join(texts)

async def generate_summary(content):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é OpenAI"""
    if not OPENAI_API_KEY or not content:
        return ""
    
    try:
        client = AsyncOpenAI(api_key=OPENAI_API_KEY)
        response = await client.chat.completions.create(
            model="gpt-3.5-turbo-1106",
            messages=[
                {
                    "role": "system",
                    "content": "–¢—ã –æ–ø—ã—Ç–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π. –°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –∏–∑ —Ç–µ–∫—Å—Ç–∞, –≤—ã–¥–µ–ª–∏–≤ —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
                },
                {
                    "role": "user",
                    "content": f"–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–ª–∏–Ω–æ–π –Ω–µ –±–æ–ª–µ–µ 200 —Å–∏–º–≤–æ–ª–æ–≤:\n\n{content[:10000]}"
                }
            ],
            max_tokens=150,
            temperature=0.5
        )
        
        summary = response.choices[0].message.content.strip()
        return f"\n\nüìå **–ö—Ä–∞—Ç–∫–æ:** {summary}"
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫—Ä–∞—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è: {e}")
        return ""

async def parse_article(url, source_config):
    """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç—å–∏ —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º –Ω–∞ —Ä—É—Å—Å–∫–∏–π"""
    try:
        REQUEST_COUNT.inc()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ URL —Å—Ç–∞—Ç—å–∏
        if not is_valid_url(url):
            logger.error(f"–ù–µ–≤–µ—Ä–Ω—ã–π URL —Å—Ç–∞—Ç—å–∏: {url}")
            return None
            
        async with await create_async_session() as session:
            # –°–ª—É—á–∞–π–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            headers = {
                'User-Agent': random.choice(USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Referer': random.choice(REFERERS),
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1'
            }
            
            # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞
            delay = random.uniform(1.0, 3.0)
            await asyncio.sleep(delay)
            
            try:
                response = await session.get(url, headers=headers)
                response.raise_for_status()
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ ({url}): {e}")
                return None
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            soup = BeautifulSoup(response.content, DEFAULT_PARSER)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å –æ—á–∏—Å—Ç–∫–æ–π
            title = soup.title.string.strip() if soup.title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
            title = clean_title(title)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            article_content = extract_main_content(soup)
            
            # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            if len(article_content or "") < 100:
                logger.warning(f"–ö–æ–Ω—Ç–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π: {url}")
                PARSE_FAILURE.inc()
                return None
                
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            article_content = clean_content(article_content, source_config['name'])
            
            # –ü–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–∏–π –µ—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π
            if source_config.get('lang', 'en') != 'ru':
                title = await translate_to_russian(title)
                article_content = await translate_to_russian(article_content)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keywords = extract_keywords(f"{title} {article_content}")
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏
            importance, importance_level = analyze_importance(title, article_content)
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–≥–æ–≤
            tags = list(set(source_config['tags'] + keywords))[:5]
            
            # –•–µ—à–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–æ–≤
            title_hash = hashlib.md5(title.encode('utf-8')).hexdigest()
            content_hash = hashlib.md5(article_content[:3000].encode('utf-8')).hexdigest()
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
            summary = await generate_summary(article_content) if OPENAI_API_KEY else ""
            
            PARSE_SUCCESS.inc()
            return {
                'title': title,
                'content': article_content,
                'url': url,
                'importance': importance,
                'importance_level': importance_level,
                'category': source_config['category'],
                'tags': tags,
                'source': source_config['name'],
                'content_hash': content_hash,
                'title_hash': title_hash,
                'trust_score': source_config.get('trust_score', 5),
                'publish_date': datetime.now(timezone.utc),
                'summary': summary
            }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏ {url}: {type(e).__name__} - {str(e)}", exc_info=True)
        PARSE_FAILURE.inc()
        return None

def format_post(article):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å—Ç–∞ –¥–ª—è Telegram"""
    try:
        max_length = 3000
        if len(article['content']) > max_length:
            content = article['content'][:max_length] + "..."
        else:
            content = article['content']
        
        tags_str = " ".join([f"#{tag.replace(' ', '_')}" for tag in article['tags']])
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –¥–æ–≤–µ—Ä–∏—è
        trust_emoji = "üîπ"
        if article['trust_score'] >= 9:
            trust_emoji = "üî∂"
        elif article['trust_score'] >= 7:
            trust_emoji = "üî∑"
        
        message = (
            f"{article['importance']} **{article['title']}**\n\n"
            f"‚ÑπÔ∏è **–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {article['category']}\n"
            f"üì∞ **–ò—Å—Ç–æ—á–Ω–∏–∫:** {article['source']} {trust_emoji}\n\n"
            f"{content}{article.get('summary', '')}\n\n"
            f"üîó [–û—Ä–∏–≥–∏–Ω–∞–ª —Å—Ç–∞—Ç—å–∏]({article['url']})\n\n"
            f"{tags_str}"
        )
        
        return message
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å—Ç–∞: {e}")
        return None

async def fetch_articles(source_config):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        async with await create_async_session() as session:
            url = source_config.get('rss_url', source_config['url'])
            
            if not url:
                logger.error(f"URL –Ω–µ —É–∫–∞–∑–∞–Ω –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_config['name']}")
                return []
                
            logger.info(f"–ó–∞–ø—Ä–æ—Å –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É [{source_config['name']}]: {url}")
            
            headers = {
                'User-Agent': random.choice(USER_AGENTS),
                'Accept': 'application/xml, text/xml, */*;q=0.9',
                'Referer': random.choice(REFERERS),
                'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
                'Connection': 'keep-alive'
            }
            
            try:
                response = await session.get(url, headers=headers)
                response.raise_for_status()
            except httpx.RequestError as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ {url}: {e}")
                return []
            
            # –ü–∞—Ä—Å–∏–Ω–≥ RSS
            soup = BeautifulSoup(response.content, 'xml')
            
            items = []
            if soup.find('rss'):
                items = soup.find_all('item')
            elif soup.find('feed'):
                items = soup.find_all('entry')
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ RSS {source_config['name']}")
            
            articles = []
            for item in items[:10]:
                link = None
                
                if item.find('link'):
                    link_elem = item.find('link')
                    if link_elem.get('href'):
                        link = link_elem.get('href').strip()
                    elif link_elem.text:
                        link = link_elem.text.strip()
                elif item.find('guid'):
                    link = item.find('guid').text.strip()
                
                if not link or not is_valid_url(link):
                    continue
                    
                articles.append(link)
            
            return articles
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–µ–π: {e}", exc_info=True)
        return []

async def generate_analytics_report():
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 12 —á–∞—Å–æ–≤"""
    try:
        recent_data = db.get_recent_analytics(12)
        
        if not recent_data:
            return "–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 12 —á–∞—Å–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
        
        category_stats = db.get_category_stats(12)
        keyword_freq = db.get_keyword_frequency(12, 5)
        
        importance_count = {1: 0, 2: 0, 3: 0}
        for item in recent_data:
            importance_count[item['importance']] += 1
        
        report = "üìä **–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 12 —á–∞—Å–æ–≤**\n\n"
        report += f"üì∞ –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(recent_data)}\n"
        report += f"üî• –°—Ä–æ—á–Ω—ã—Ö: {importance_count[3]}\n"
        report += f"‚ùó –í–∞–∂–Ω—ã—Ö: {importance_count[2]}\n"
        report += f"üìå –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö: {importance_count[1]}\n\n"
        
        report += "**–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:**\n"
        for category, count in category_stats.items():
            report += f"- {category}: {count} –Ω–æ–≤–æ—Å—Ç–µ–π\n"
        
        report += "\n**–¢–æ–ø –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤:**\n"
        for keyword, count in keyword_freq:
            report += f"#{keyword.replace(' ', '_')} ({count}), "
        
        report = report.rstrip(", ") + "\n\n"
        
        report += "**–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥:**\n"
        if importance_count[3] > 5:
            report += "–í—ã—Å–æ–∫–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è —Å—Ä–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–∞–ø—Ä—è–∂–µ–Ω–Ω—É—é —Å–∏—Ç—É–∞—Ü–∏—é –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º —Å–µ–∫—Ç–æ—Ä–µ. "
        elif importance_count[3] > 2:
            report += "–ù–∞–ª–∏—á–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ä–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å–∏–≥–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ –ø–æ–≤—ã—à–µ–Ω–Ω–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–æ–≤. "
        else:
            report += "–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ä–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –≥–æ–≤–æ—Ä–∏—Ç –æ–± –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏. "
        
        report += "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–±—Ä–∞—Ç–∏—Ç—å –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –æ—Ç—á–µ—Ç—ã –∫—Ä—É–ø–Ω—ã—Ö –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–π –∏ –¥–∞–Ω–Ω—ã–µ –ø–æ –∏–Ω—Ñ–ª—è—Ü–∏–∏."
        
        return report
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
        return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç"

async def send_error_notification(message):
    """–û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ –≤ Telegram"""
    try:
        client = TelegramClient('error_notifier', API_ID, API_HASH)
        await client.start(bot_token=BOT_TOKEN_PUBLISHER)
        await client.send_message(ADMIN_CHAT_ID, f"üö® –û—à–∏–±–∫–∞: {message}")
        await client.disconnect()
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ: {e}")

async def send_morning_briefing(publisher):
    """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ —É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –±—Ä–∏—Ñ–∏–Ω–≥–∞"""
    try:
        logger.info("–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –±—Ä–∏—Ñ–∏–Ω–≥–∞...")
        report = await generate_analytics_report()
        await publisher.send_message(CHANNEL, report, parse_mode='markdown')
        logger.info("–£—Ç—Ä–µ–Ω–Ω–∏–π –±—Ä–∏—Ñ–∏–Ω–≥ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –±—Ä–∏—Ñ–∏–Ω–≥–∞: {e}")

# ================== –ö–û–î –ë–û–¢–ê-–°–ë–û–†–©–ò–ö–ê ==================

async def run_collector():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞-—Å–±–æ—Ä—â–∏–∫–∞"""
    logger.info("–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞-—Å–±–æ—Ä—â–∏–∫–∞...")
    
    try:
        collector = TelegramClient('collector_session', API_ID, API_HASH)
        await collector.start(bot_token=BOT_TOKEN_COLLECTOR)
        logger.info("–ë–æ—Ç-—Å–±–æ—Ä—â–∏–∫ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω")
        await collector.send_message(ADMIN_CHAT_ID, "üîÑ –ë–æ—Ç-—Å–±–æ—Ä—â–∏–∫ –∑–∞–ø—É—â–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π
        @collector.on(events.NewMessage(pattern=r'https?://\S+'))
        async def handle_link(event):
            try:
                url = event.message.text.strip()
                if not is_valid_url(url):
                    await event.reply("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç URL")
                    return
                    
                logger.info(f"–ü–æ–ª—É—á–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {url}")
                
                db.add_pending_link(
                    url=url,
                    source='user_submission',
                    added_by=event.sender_id,
                    priority=3  # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                )
                
                await event.reply("‚úÖ –°—Å—ã–ª–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É!")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Å—ã–ª–∫–∏: {e}")
        
        @collector.on(events.NewMessage(from_users=ADMIN_CHAT_ID, pattern='/stats'))
        async def handle_stats(event):
            try:
                report = await generate_analytics_report()
                await event.reply(report, parse_mode='markdown')
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
        
        @collector.on(events.NewMessage(from_users=ADMIN_CHAT_ID, pattern='/collect'))
        async def handle_collect(event):
            try:
                await collect_links()
                await event.reply("‚úÖ –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω!")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫: {e}")
        
        async def collect_links():
            logger.info("–ù–∞—á–∞–ª–æ —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫")
            pending_urls = {item['url'] for item in db.get_pending_links(limit=1000)}
            
            for source in SOURCES_CONFIG:
                try:
                    articles = await fetch_articles(source)
                    new_count = 0
                    for url in articles:
                        if url in pending_urls:
                            continue
                        db.add_pending_link(
                            url=url,
                            source=source['name'],
                            priority=2  # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                        )
                        pending_urls.add(url)
                        new_count += 1
                    logger.info(f"–î–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source['name']} –¥–æ–±–∞–≤–ª–µ–Ω–æ {new_count} –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫")
                    await asyncio.sleep(1)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫ –∏–∑ {source['name']}: {e}")
            
            logger.info(f"–í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –≤ –æ—á–µ—Ä–µ–¥–∏: {len(pending_urls)}")
            await collector.send_message(ADMIN_CHAT_ID, f"‚úÖ –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω! –í –æ—á–µ—Ä–µ–¥–∏: {len(pending_urls)} —Å—Å—ã–ª–æ–∫")
        
        async def scheduled_collection():
            logger.info("–ó–∞–ø—É—Å–∫ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—á–µ—Ä–µ–¥–∏")
            while True:
                try:
                    await collect_links()
                    QUEUE_SIZE.set(len(db.get_pending_links(limit=1000)))
                    await asyncio.sleep(1800)  # 30 –º–∏–Ω—É—Ç
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –≤ scheduled_collection: {e}")
                    await asyncio.sleep(60)
        
        asyncio.create_task(scheduled_collection())
        
        logger.info("–ë–æ—Ç-—Å–±–æ—Ä—â–∏–∫ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        await collector.run_until_disconnected()
        
    except errors.RPCError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è Telegram: {e}")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ —Å–±–æ—Ä—â–∏–∫–µ: {e}", exc_info=True)

# ================== –ö–û–î –ë–û–¢–ê-–ü–£–ë–õ–ò–ö–ê–¢–û–†–ê ==================

async def run_publisher():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞-–ø—É–±–ª–∏–∫–∞—Ç–æ—Ä–∞"""
    logger.info("–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞-–ø—É–±–ª–∏–∫–∞—Ç–æ—Ä–∞...")
    
    try:
        publisher = TelegramClient('publisher_session', API_ID, API_HASH)
        await publisher.start(bot_token=BOT_TOKEN_PUBLISHER)
        logger.info("–ë–æ—Ç-–ø—É–±–ª–∏–∫–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω")
        await publisher.send_message(ADMIN_CHAT_ID, "üöÄ –ë–æ—Ç-–ø—É–±–ª–∏–∫–∞—Ç–æ—Ä –∑–∞–ø—É—â–µ–Ω!")
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π
        @publisher.on(events.NewMessage)
        async def track_engagement(event):
            """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏"""
            if event.chat_id == int(CHANNEL):
                if event.reply_to_msg_id:
                    # –†–µ–∞–∫—Ü–∏–∏ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
                    db.update_engagement(event.reply_to_msg_id, 0.5)
                elif event.views:
                    # –ü—Ä–æ—Å–º–æ—Ç—Ä—ã
                    db.update_engagement(event.id, event.views / 1000)
        
        @publisher.on(events.NewMessage(from_users=ADMIN_CHAT_ID, pattern='/publish'))
        async def handle_publish(event):
            try:
                await event.reply("‚è≥ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—á–µ—Ä–µ–¥–∏...")
                await process_pending_links(publisher)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Ä—É—á–Ω–æ–π –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")
        
        @publisher.on(events.NewMessage(from_users=ADMIN_CHAT_ID, pattern='/clean'))
        async def handle_clean(event):
            try:
                db.clear_pending_links()
                await event.reply("‚úÖ –û—á–µ—Ä–µ–¥—å –æ—á–∏—â–µ–Ω–∞!")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –æ—á–µ—Ä–µ–¥–∏: {e}")
        
        @publisher.on(events.NewMessage(from_users=ADMIN_CHAT_ID, pattern=r'/post\s+(https?://\S+)'))
        async def handle_immediate_post(event):
            try:
                url = event.pattern_match.group(1).strip()
                if not is_valid_url(url):
                    await event.reply("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π URL")
                    return
                    
                # –î–æ–±–∞–≤–ª—è–µ–º —Å –≤—ã—Å–æ–∫–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
                db.add_pending_link(
                    url=url,
                    source='immediate_command',
                    added_by=event.sender_id,
                    priority=5  # –ù–∞–∏–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                )
                await event.reply("‚úÖ –°—Å—ã–ª–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –æ—á–µ—Ä–µ–¥—å —Å –≤—ã—Å–æ–∫–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º!")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å—Å—ã–ª–∫–∏: {e}")
        
        async def process_pending_links(client):
            try:
                pending = db.get_pending_links(limit=15)
                if not pending:
                    logger.info("–ù–µ—Ç —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                    await client.send_message(ADMIN_CHAT_ID, "‚ÑπÔ∏è –ù–µ—Ç —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                    return
                
                logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {len(pending)} —Å—Å—ã–ª–æ–∫")
                await client.send_message(ADMIN_CHAT_ID, f"üîç –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(pending)} —Å—Å—ã–ª–æ–∫...")
                
                processed = []
                errors = []
                success_count = 0
                
                for item in pending:
                    try:
                        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Å—ã–ª–∫–∏: {item['url']}")
                        
                        if item['source'] == 'user_submission':
                            source_config = {
                                'name': '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è —Å—Å—ã–ª–∫–∞',
                                'category': '—Ä–∞–∑–Ω–æ–µ',
                                'tags': ['–Ω–æ–≤–æ—Å—Ç–∏'],
                                'lang': 'ru',
                                'trust_score': 5
                            }
                        else:
                            source_config = next(
                                (s for s in SOURCES_CONFIG if s['name'] == item['source']), 
                                {
                                    'name': item['source'],
                                    'category': '—Ä–∞–∑–Ω–æ–µ',
                                    'tags': ['–Ω–æ–≤–æ—Å—Ç–∏'],
                                    'lang': 'ru',
                                    'trust_score': 5
                                }
                            )
                        
                        article = await parse_article(item['url'], source_config)
                        if not article:
                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç–∞—Ç—å—é: {item['url']}")
                            errors.append(item['url'])
                            continue
                        
                        is_duplicate = False
                        
                        if db.is_link_posted(item['url']):
                            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç –ø–æ URL: {item['url']}")
                            is_duplicate = True
                        
                        elif db.is_content_hash_exists(article['content_hash']):
                            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É: {article['title'][:50]}...")
                            is_duplicate = True
                        
                        elif db.is_title_hash_recent(article['title_hash']):
                            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω –≤–æ–∑–º–æ–∂–Ω—ã–π –¥—É–±–ª–∏–∫–∞—Ç –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É: {article['title']}")
                            is_duplicate = True
                        
                        if is_duplicate:
                            processed.append(item['id'])
                            continue
                        
                        message = format_post(article)
                        if not message:
                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å—Ç –¥–ª—è: {item['url']}")
                            errors.append(item['url'])
                            continue
                        
                        try:
                            await client.send_message(
                                entity=CHANNEL,
                                message=message,
                                link_preview=False,
                                parse_mode="markdown"
                            )
                            PUBLISH_COUNT.inc()
                            success_count += 1
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
                            analytics_entry = {
                                "url": item['url'],
                                "title": article['title'],
                                "category": article['category'],
                                "tags": article['tags'],
                                "importance_level": article['importance_level'],
                                "source": article['source'],
                                "content_hash": article['content_hash'],
                                "title_hash": article['title_hash'],
                                "timestamp": datetime.now(timezone.utc).isoformat(),
                                "summary": article.get('summary', ''),
                                "trust_score": article['trust_score']
                            }
                            db.add_analytics(analytics_entry)
                            
                            # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω–æ–µ
                            db.add_posted_link(item['url'])
                            db.add_content_hash(article['content_hash'])
                            db.add_title_hash(article['title_hash'])
                            
                        except errors.RPCError as e:
                            logger.error(f"–û—à–∏–±–∫–∞ Telegram: {e}")
                            errors.append(item['url'])
                            continue
                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")
                            errors.append(item['url'])
                            continue
                        
                        processed.append(item['id'])
                        logger.info(f"–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {article['title'][:50]}...")
                        
                        await asyncio.sleep(random.uniform(3, 8))
                    
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Å—ã–ª–∫–∏ {item['url']}: {e}")
                        errors.append(item['url'])
                
                # –£–¥–∞–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                for item_id in processed:
                    db.remove_pending_link(item_id)
                
                report = (
                    f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\n"
                    f"‚Ä¢ –£—Å–ø–µ—à–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {success_count}\n"
                    f"‚Ä¢ –û—à–∏–±–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(errors)}\n"
                    f"‚Ä¢ –î—É–±–ª–∏–∫–∞—Ç–æ–≤/–ø—Ä–æ–ø—É—â–µ–Ω–æ: {len(processed) - success_count}\n"
                    f"‚Ä¢ –û—Å—Ç–∞–ª–æ—Å—å –≤ –æ—á–µ—Ä–µ–¥–∏: {len(db.get_pending_links(limit=1000))}"
                )
                await client.send_message(ADMIN_CHAT_ID, report)
                logger.info(report)
                
                if errors:
                    error_list = "\n".join(f"- {url}" for url in errors[:10])
                    if len(errors) > 10:
                        error_list += f"\n... –∏ –µ—â—ë {len(errors) - 10} –æ—à–∏–±–æ–∫"
                    await client.send_message(
                        ADMIN_CHAT_ID, 
                        f"‚ùå –°—Å—ã–ª–∫–∏ —Å –æ—à–∏–±–∫–∞–º–∏:\n{error_list}",
                        parse_mode=None
                    )
            except Exception as e:
                logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ process_pending_links: {e}", exc_info=True)
        
        async def scheduled_processing():
            logger.info("–ó–∞–ø—É—Å–∫ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—á–µ—Ä–µ–¥–∏")
            while True:
                try:
                    await process_pending_links(publisher)
                    QUEUE_SIZE.set(len(db.get_pending_links(limit=1000)))
                    
                    # –£—Ç—Ä–µ–Ω–Ω–∏–π –±—Ä–∏—Ñ–∏–Ω–≥ –≤ 8:00 –ø–æ –ú–°–ö (UTC+3)
                    now_utc = datetime.now(timezone.utc)
                    moscow_tz = timezone(timedelta(hours=3))
                    moscow_time = now_utc.astimezone(moscow_tz)
                    
                    if moscow_time.hour == 8 and moscow_time.minute == 0:
                        await send_morning_briefing(publisher)
                    
                    await asyncio.sleep(300)  # 5 –º–∏–Ω—É—Ç
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –≤ scheduled_processing: {e}")
                    await asyncio.sleep(60)
        
        asyncio.create_task(scheduled_processing())
        
        logger.info("–ë–æ—Ç-–ø—É–±–ª–∏–∫–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        await publisher.run_until_disconnected()
        
    except errors.RPCError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è Telegram: {e}")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –ø—É–±–ª–∏–∫–∞—Ç–æ—Ä–µ: {e}", exc_info=True)

# ================== –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ==================

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–æ–≤"""
    tasks = [
        asyncio.create_task(run_collector()),
        asyncio.create_task(run_publisher())
    ]
    await asyncio.gather(*tasks, return_exceptions=True)

if __name__ == "__main__":
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        logger.info("–ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã —Å–±–æ—Ä–∞ –∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π")
        loop.run_until_complete(main())
    except KeyboardInterrupt:
        logger.info("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–∏—Å—Ç–µ–º—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
    finally:
        logger.info("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã")